{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 CSV files found in folder: 10\n",
      "Valid abnormal files: ['768_.csv', '225_.csv', '332_340_350_388_389_402_410_467_481_515_542_.csv', '7_37_72_128_.csv', '436_.csv', '241_281_375_385_.csv', '22_36_54_142_152_164_192_257_258_276_282_301_334_416_455_487_508_588_597_645_745_811_853_882_916_.csv', '894_923_931_962_.csv', '341_501_.csv', '514_549_568_621_704_718_799_820_836_868_878_889_897_899_921_932_946_954_959_.csv', '232_249_285_304_344_418_.csv', '468_507_529_647_.csv', '823_.csv', '151_.csv', '421_.csv', '13_30_35_56_62_71_93_98_105_126_165_211_289_295_312_325_365_372_379_391_406_427_.csv', '23_75_119_160_231_245_278_308_321_682_708_769_843_864_.csv', '220_263_300_.csv', '835_.csv', '964_.csv', '493_544_.csv', '48_122_132_137_161_172_329_358_408_497_601_755_859_896_940_.csv', '20_47_73_141_229_322_359_420_447_506_577_602_607_699_767_.csv', '563_590_651_663_.csv', '370_.csv', '963_.csv', '620_.csv', '874_.csv', '430_523_550_555_571_.csv', '361_.csv', '382_423_.csv', '106_123_183_207_.csv', '825_.csv', '589_.csv', '526_634_638_658_.csv', '675_723_729_739_761_.csv', '877_890_949_.csv', '585_.csv', '81_109_144_177_260_303_343_352_393_457_.csv', '384_.csv', '331_351_367_437_.csv', '396_.csv', '12_689_721_850_.csv', '182_.csv', '800_861_957_.csv', '202_283_495_.csv', '486_.csv', '536_567_576_691_715_837_.csv', '11_77_97_203_226_237_274_.csv', '591_.csv', '19_94_.csv', '88_127_149_184_327_338_400_474_494_604_667_966_.csv', '641_661_676_684_700_720_.csv', '736_791_886_.csv', '727_753_774_785_831_.csv']\n",
      "Number of valid abnormal files: 55\n",
      "Valid normal files: ['758_.csv', '347_.csv', '143_.csv', '569_.csv', '480_.csv', '5_.csv', '292_.csv', '277_.csv', '16_.csv', '958_.csv', '21_31_42_134_.csv', '14_40_64_.csv', '214_.csv', '336_404_428_434_492_.csv', '452_.csv', '619_.csv', '632_793_888_.csv', '18_.csv', '464_477_562_570_622_625_.csv', '34_.csv', '644_.csv', '1_.csv', '4_.csv', '78_.csv', '592_626_648_.csv', '174_235_.csv', '193_.csv', '114_157_223_238_.csv', '51_.csv', '99_.csv', '955_.csv', '636_694_717_764_.csv', '751_.csv', '826_881_902_929_.csv', '687_.csv', '92_116_121_.csv', '227_.csv', '933_.csv', '135_228_.csv', '397_.csv', '255_.csv', '63_.csv', '200_.csv', '275_.csv', '713_.csv', '43_79_112_120_148_.csv', '511_.csv', '3_.csv', '442_453_.csv', '66_.csv', '789_.csv', '213_.csv', '355_.csv', '335_360_403_637_650_707_.csv', '15_52_.csv', '860_.csv', '938_.csv', '194_317_.csv', '85_.csv', '693_.csv', '609_.csv', '967_.csv', '617_624_.csv', '939_944_.csv', '968_.csv', '8_.csv', '482_498_527_547_578_599_.csv', '6_.csv', '798_817_.csv', '856_.csv', '96_.csv', '852_872_907_917_.csv', '551_574_.csv', '82_.csv', '554_.csv', '324_.csv', '431_.csv', '965_.csv', '390_414_.csv', '108_.csv', '9_76_111_314_432_471_733_752_805_842_857_887_.csv', '209_.csv', '61_.csv', '17_29_90_.csv', '131_.csv', '10_.csv', '87_145_156_171_189_198_.csv', '46_.csv', '748_.csv', '2_.csv', '32_.csv', '368_.csv', '265_309_.csv', '49_.csv', '766_.csv', '660_747_.csv', '605_.csv', '444_502_531_.csv', '895_.csv', '947_.csv', '970_.csv', '378_.csv', '732_812_915_.csv', '342_.csv']\n",
      "Number of valid normal files: 104\n",
      "Loaded 130 trajectories from folder 10.\n",
      "Using eps=2.64 and min_samples=6\n",
      "Found 2 clusters (including noise):\n",
      "  Cluster -1: 3 trajectories\n",
      "  Cluster 0: 127 trajectories\n",
      "Accuracy: 0.6000\n",
      "Confusion Matrix:\n",
      "[[77  2]\n",
      " [50  1]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.97      0.75        79\n",
      "           1       0.33      0.02      0.04        51\n",
      "\n",
      "    accuracy                           0.60       130\n",
      "   macro avg       0.47      0.50      0.39       130\n",
      "weighted avg       0.50      0.60      0.47       130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Temp\\ipykernel_41076\\2369266870.py:228: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = plt.cm.get_cmap('tab10', max(2, len(unique_clusters)-1))\n",
      "C:\\Users\\abhis\\AppData\\Local\\Temp\\ipykernel_41076\\2369266870.py:277: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = plt.cm.get_cmap('tab10', max(2, len(np.unique(clusters))-1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved to trajectory_images\\trajectory_visualization_10.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntuning_results = tune_dbscan_parameters(dist_matrix, labels)\\ntuning_results.to_csv(os.path.join(output_dir, f\"parameter_tuning_{sample}.csv\"), index=False)\\n\\n# Find best parameters by accuracy\\nbest_params = tuning_results.loc[tuning_results[\\'accuracy\\'].idxmax()]\\nprint(\"Best parameters by accuracy:\")\\nprint(best_params)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.preprocessing import TimeSeriesResampler\n",
    "\n",
    "###############################################################################\n",
    "# Parameters & Paths\n",
    "###############################################################################\n",
    "sample = \"10\"  # Process folder \"10\"\n",
    "folder_path = sample            # Folder containing CSV files for sample 10\n",
    "abnormal_txt_file = f\"AbnormalTracks_{sample}.txt\"  # Text file with abnormal track IDs\n",
    "image_path = f\"sample{sample}.jpg\"  # Background image for visualization (optional)\n",
    "\n",
    "# Create output directory if needed\n",
    "output_dir = \"trajectory_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 1. Read Abnormal Track IDs and Determine Valid Files\n",
    "###############################################################################\n",
    "with open(abnormal_txt_file, \"r\") as file:\n",
    "    abnormal_lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "    abnormal_files_names = [f.split(\".\")[0] for f in abnormal_lines]\n",
    "\n",
    "# List all CSV files available in the folder.\n",
    "all_csv_files = set(os.listdir(folder_path))\n",
    "print(len(all_csv_files), \"CSV files found in folder:\", folder_path)\n",
    "\n",
    "# For each abnormal track ID, perform a prefix match to find a valid CSV file.\n",
    "valid_abnormal_files = []\n",
    "for name in abnormal_files_names:\n",
    "    for csv_file_name in all_csv_files.copy():\n",
    "        if csv_file_name.endswith(\".csv\") and name in csv_file_name:\n",
    "            valid_abnormal_files.append(csv_file_name)\n",
    "            all_csv_files.remove(csv_file_name)  # Remove matched file so remaining are normal\n",
    "            break  # Use first match found\n",
    "\n",
    "# Normal files are those CSVs that are not marked as abnormal.\n",
    "valid_normal_files = [f for f in all_csv_files if f.endswith('.csv') and f not in valid_abnormal_files]\n",
    "\n",
    "print(\"Valid abnormal files:\", valid_abnormal_files)\n",
    "print(\"Number of valid abnormal files:\", len(valid_abnormal_files))\n",
    "print(\"Valid normal files:\", valid_normal_files)\n",
    "print(\"Number of valid normal files:\", len(valid_normal_files))\n",
    "\n",
    "###############################################################################\n",
    "# 2. Define Functions to Extract Trajectories & Preprocess\n",
    "###############################################################################\n",
    "def extract_trajectory(csv_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file (with columns: frameNo, left, top, w, h) and returns the\n",
    "    trajectory as an array of [frameNo, center_x, center_y] values.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, usecols=[\"frameNo\", \"left\", \"top\", \"w\", \"h\"])\n",
    "    df[\"center_x\"] = df[\"left\"] + df[\"w\"] / 2\n",
    "    df[\"center_y\"] = df[\"top\"] + df[\"h\"] / 2\n",
    "    return df[[\"frameNo\", \"center_x\", \"center_y\"]].values\n",
    "\n",
    "def normalize_trajectory(traj):\n",
    "    \"\"\"\n",
    "    Normalize a trajectory by subtracting the starting point and scaling\n",
    "    to make the features more comparable.\n",
    "    \"\"\"\n",
    "    # Extract just the spatial coordinates\n",
    "    spatial = traj[:, 1:3]\n",
    "    \n",
    "    # Center trajectory by subtracting the first point\n",
    "    centered = spatial - spatial[0]\n",
    "    \n",
    "    # Calculate the max distance from origin for scaling\n",
    "    max_dist = np.max(np.linalg.norm(centered, axis=1))\n",
    "    if max_dist > 0:\n",
    "        # Normalize to range [0, 1]\n",
    "        normalized = centered / max_dist\n",
    "    else:\n",
    "        normalized = centered\n",
    "    \n",
    "    # Return with original frameNo\n",
    "    result = np.column_stack((traj[:, 0], normalized))\n",
    "    return result\n",
    "\n",
    "def resample_trajectory(traj, n_points=50):\n",
    "    \"\"\"\n",
    "    Resamples the trajectory to a fixed number of points using tslearn's TimeSeriesResampler.\n",
    "    Also normalizes the trajectory for better comparison.\n",
    "    \"\"\"\n",
    "    # First normalize the trajectory\n",
    "    traj_norm = normalize_trajectory(traj)\n",
    "    \n",
    "    # Prepare for tslearn (requires 3D shape: [n_samples, n_timesteps, n_features])\n",
    "    traj_3d = traj_norm.reshape(1, traj_norm.shape[0], traj_norm.shape[1])\n",
    "    \n",
    "    # Resample to fixed length\n",
    "    resampler = TimeSeriesResampler(sz=n_points)\n",
    "    traj_resampled = resampler.fit_transform(traj_3d)\n",
    "    \n",
    "    # Return the first (and only) sample\n",
    "    return traj_resampled[0]\n",
    "\n",
    "###############################################################################\n",
    "# 3. Load Trajectories & Prepare Labels\n",
    "###############################################################################\n",
    "raw_trajectories = []  # Store original trajectories\n",
    "processed_trajectories = []  # Store processed trajectories for clustering\n",
    "labels = []        # 1 for abnormal, 0 for normal (based on file designation)\n",
    "file_names = []    # Store file names for reference\n",
    "\n",
    "# Process abnormal CSV files\n",
    "for file_name in valid_abnormal_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    traj = extract_trajectory(file_path)\n",
    "    if len(traj) > 5:  # Ensure trajectory has enough points\n",
    "        raw_trajectories.append(traj)\n",
    "        processed_trajectories.append(resample_trajectory(traj, n_points=50))\n",
    "        labels.append(1)  # Abnormal\n",
    "        file_names.append(file_name)\n",
    "\n",
    "# Process normal CSV files\n",
    "for file_name in valid_normal_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    traj = extract_trajectory(file_path)\n",
    "    if len(traj) > 5:  # Ensure trajectory has enough points\n",
    "        raw_trajectories.append(traj)\n",
    "        processed_trajectories.append(resample_trajectory(traj, n_points=50))\n",
    "        labels.append(0)  # Normal\n",
    "        file_names.append(file_name)\n",
    "\n",
    "print(f\"Loaded {len(processed_trajectories)} trajectories from folder {folder_path}.\")\n",
    "\n",
    "###############################################################################\n",
    "# 4. Compute DTW-based distance matrix for trajectories\n",
    "###############################################################################\n",
    "def compute_dtw_matrix(trajectories):\n",
    "    \"\"\"\n",
    "    Compute a distance matrix for trajectories using Dynamic Time Warping (DTW).\n",
    "    Only compares the spatial coordinates (x, y) and ignores the time component.\n",
    "    \"\"\"\n",
    "    n = len(trajectories)\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            # Extract just the spatial dimensions for DTW\n",
    "            seq1 = trajectories[i][:, 1:3]  # x, y coordinates\n",
    "            seq2 = trajectories[j][:, 1:3]  # x, y coordinates\n",
    "            \n",
    "            # Compute DTW distance\n",
    "            distance = dtw(seq1, seq2)\n",
    "            \n",
    "            # Fill the symmetric matrix\n",
    "            dist_matrix[i, j] = distance\n",
    "            dist_matrix[j, i] = distance\n",
    "    \n",
    "    return dist_matrix\n",
    "\n",
    "# Compute distance matrix\n",
    "dist_matrix = compute_dtw_matrix(processed_trajectories)\n",
    "\n",
    "# Visualize the distance matrix (optional)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(dist_matrix, cmap='viridis')\n",
    "plt.colorbar(label='DTW Distance')\n",
    "plt.title('DTW Distance Matrix between Trajectories')\n",
    "plt.savefig(os.path.join(output_dir, f\"dtw_distance_matrix_{sample}.png\"))\n",
    "plt.close()\n",
    "\n",
    "###############################################################################\n",
    "# 5. Apply DBSCAN for Clustering using the precomputed DTW distance matrix\n",
    "###############################################################################\n",
    "# The eps value needs to be tuned based on the distribution of distances\n",
    "# Start with the median of the distances\n",
    "eps_value = np.median(dist_matrix) * 0.5  # 50% of median distance as starting point\n",
    "min_samples = max(3, int(len(processed_trajectories) * 0.05))  # At least 5% of total trajectories\n",
    "\n",
    "print(f\"Using eps={eps_value:.2f} and min_samples={min_samples}\")\n",
    "\n",
    "# Apply DBSCAN with precomputed distance matrix\n",
    "dbscan = DBSCAN(\n",
    "    eps=eps_value, \n",
    "    min_samples=min_samples,\n",
    "    metric='precomputed'\n",
    ")\n",
    "clusters = dbscan.fit_predict(dist_matrix)\n",
    "\n",
    "# Count clusters\n",
    "unique_clusters = np.unique(clusters)\n",
    "print(f\"Found {len(unique_clusters)} clusters (including noise):\")\n",
    "for c in unique_clusters:\n",
    "    count = np.sum(clusters == c)\n",
    "    print(f\"  Cluster {c}: {count} trajectories\")\n",
    "\n",
    "# Predicted labels: trajectories labeled as noise (cluster == -1) are abnormal\n",
    "predicted_labels = [1 if cl == -1 else 0 for cl in clusters]\n",
    "\n",
    "###############################################################################\n",
    "# 6. Evaluate clustering results\n",
    "###############################################################################\n",
    "# Compare to ground truth labels\n",
    "accuracy = np.mean(np.array(predicted_labels) == np.array(labels))\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(labels, predicted_labels)\n",
    "cr = classification_report(labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Classification Report:\")\n",
    "print(cr)\n",
    "\n",
    "# Create a DataFrame with results\n",
    "results_df = pd.DataFrame({\n",
    "    'filename': file_names,\n",
    "    'true_label': labels,\n",
    "    'cluster': clusters,\n",
    "    'predicted_label': predicted_labels\n",
    "})\n",
    "results_df.to_csv(os.path.join(output_dir, f\"clustering_results_{sample}.csv\"), index=False)\n",
    "\n",
    "###############################################################################\n",
    "# 7. Visualize clusters\n",
    "###############################################################################\n",
    "# Color map for clusters (excluding noise)\n",
    "cmap = plt.cm.get_cmap('tab10', max(2, len(unique_clusters)-1))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# First plot the clusters (excluding noise points)\n",
    "for i, traj in enumerate(processed_trajectories):\n",
    "    if clusters[i] == -1:\n",
    "        continue  # Skip noise points for now\n",
    "    \n",
    "    color = cmap(clusters[i] % 10)  # Cycle through 10 colors\n",
    "    plt.plot(traj[:, 1], traj[:, 2], \n",
    "             color=color, alpha=0.5, \n",
    "             label=f\"Cluster {clusters[i]}\" if clusters[i] not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "# Then plot the noise points on top\n",
    "for i, traj in enumerate(processed_trajectories):\n",
    "    if clusters[i] != -1:\n",
    "        continue\n",
    "    \n",
    "    plt.plot(traj[:, 1], traj[:, 2], \n",
    "             'k--', alpha=0.7, linewidth=1,\n",
    "             label=\"Anomaly\" if \"Anomaly\" not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "plt.title(f\"Trajectory Clustering (Sample {sample})\")\n",
    "plt.xlabel(\"Normalized X\")\n",
    "plt.ylabel(\"Normalized Y\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, f\"cluster_visualization_{sample}.png\"))\n",
    "plt.close()\n",
    "\n",
    "###############################################################################\n",
    "# 8. Visualize trajectories on original image (if available)\n",
    "###############################################################################\n",
    "def visualize_trajectories_on_image(image_path, trajectories, clusters, output_path):\n",
    "    \"\"\"\n",
    "    Visualize multiple trajectories on the background image, colored by cluster.\n",
    "    \"\"\"\n",
    "    # Try to read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Image {image_path} not found. Creating a blank canvas.\")\n",
    "        # Create a blank canvas\n",
    "        max_x = max(np.max(traj[:, 1]) for traj in raw_trajectories)\n",
    "        max_y = max(np.max(traj[:, 2]) for traj in raw_trajectories)\n",
    "        image = np.ones((int(max_y) + 100, int(max_x) + 100, 3), dtype=np.uint8) * 255\n",
    "    else:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Color map for clusters\n",
    "    cmap = plt.cm.get_cmap('tab10', max(2, len(np.unique(clusters))-1))\n",
    "    \n",
    "    # Plot each trajectory\n",
    "    for i, traj in enumerate(trajectories):\n",
    "        # Get points and convert to integer coordinates\n",
    "        points = np.array([(int(p[1]), int(p[2])) for p in traj])\n",
    "        \n",
    "        # Determine color based on cluster\n",
    "        if clusters[i] == -1:\n",
    "            color = (255, 0, 0)  # Red for anomalies\n",
    "            thickness = 2\n",
    "        else:\n",
    "            # Convert colormap value to BGR for OpenCV\n",
    "            rgb = cmap(clusters[i] % 10)[:3]\n",
    "            color = tuple(int(c * 255) for c in rgb)\n",
    "            thickness = 1\n",
    "            \n",
    "        # Draw the trajectory line\n",
    "        for j in range(len(points) - 1):\n",
    "            cv2.line(image, points[j], points[j+1], color, thickness)\n",
    "        \n",
    "        # Mark start and end points\n",
    "        cv2.circle(image, points[0], 5, color, -1)  # Start point\n",
    "        cv2.circle(image, points[-1], 5, (0, 0, 0), -1)  # End point\n",
    "    \n",
    "    # Add a legend\n",
    "    legend_y = 30\n",
    "    cv2.putText(image, \"Anomaly\", (20, legend_y), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.7, (255, 0, 0), 2)\n",
    "    \n",
    "    for c in np.unique(clusters):\n",
    "        if c == -1:\n",
    "            continue\n",
    "        legend_y += 30\n",
    "        rgb = cmap(c % 10)[:3]\n",
    "        color = tuple(int(c * 255) for c in rgb)\n",
    "        cv2.putText(image, f\"Cluster {c}\", (20, legend_y), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.7, color, 2)\n",
    "    \n",
    "    # Save the result\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Visualize trajectories on image if available\n",
    "try:\n",
    "    output_path = os.path.join(output_dir, f\"trajectory_visualization_{sample}.png\")\n",
    "    visualize_trajectories_on_image(image_path, raw_trajectories, clusters, output_path)\n",
    "    print(f\"Visualization saved to {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create visualization on image: {str(e)}\")\n",
    "\n",
    "###############################################################################\n",
    "# 9. Parameter tuning helper function (optional)\n",
    "###############################################################################\n",
    "def tune_dbscan_parameters(dist_matrix, labels, eps_range=None, min_samples_range=None):\n",
    "    \"\"\"\n",
    "    Tune DBSCAN parameters by trying different combinations of eps and min_samples.\n",
    "    Returns a DataFrame with results for each parameter combination.\n",
    "    \"\"\"\n",
    "    if eps_range is None:\n",
    "        # Default: try values around the median distance\n",
    "        median_dist = np.median(dist_matrix)\n",
    "        eps_range = [median_dist * factor for factor in [0.3, 0.4, 0.5, 0.6, 0.7]]\n",
    "    \n",
    "    if min_samples_range is None:\n",
    "        # Default: try percentages of the total number of samples\n",
    "        n_samples = len(labels)\n",
    "        min_samples_range = [max(2, int(n_samples * pct)) for pct in [0.02, 0.05, 0.1, 0.15]]\n",
    "    \n",
    "    results = []\n",
    "    for eps in eps_range:\n",
    "        for min_samples in min_samples_range:\n",
    "            # Apply DBSCAN\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "            clusters = dbscan.fit_predict(dist_matrix)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            predicted = [1 if c == -1 else 0 for c in clusters]\n",
    "            accuracy = np.mean(np.array(predicted) == np.array(labels))\n",
    "            n_clusters = len(np.unique(clusters)) - (1 if -1 in clusters else 0)\n",
    "            noise_ratio = np.sum(clusters == -1) / len(clusters)\n",
    "            \n",
    "            # Append results\n",
    "            results.append({\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'n_clusters': n_clusters,\n",
    "                'noise_ratio': noise_ratio,\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Try different parameters if needed\n",
    "# Uncomment to run parameter tuning\n",
    "\"\"\"\n",
    "tuning_results = tune_dbscan_parameters(dist_matrix, labels)\n",
    "tuning_results.to_csv(os.path.join(output_dir, f\"parameter_tuning_{sample}.csv\"), index=False)\n",
    "\n",
    "# Find best parameters by accuracy\n",
    "best_params = tuning_results.loc[tuning_results['accuracy'].idxmax()]\n",
    "print(\"Best parameters by accuracy:\")\n",
    "print(best_params)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 CSV files found in folder: 10\n",
      "Valid abnormal files: ['768_.csv', '225_.csv', '332_340_350_388_389_402_410_467_481_515_542_.csv', '7_37_72_128_.csv', '436_.csv', '241_281_375_385_.csv', '22_36_54_142_152_164_192_257_258_276_282_301_334_416_455_487_508_588_597_645_745_811_853_882_916_.csv', '894_923_931_962_.csv', '341_501_.csv', '514_549_568_621_704_718_799_820_836_868_878_889_897_899_921_932_946_954_959_.csv', '232_249_285_304_344_418_.csv', '468_507_529_647_.csv', '823_.csv', '151_.csv', '421_.csv', '13_30_35_56_62_71_93_98_105_126_165_211_289_295_312_325_365_372_379_391_406_427_.csv', '23_75_119_160_231_245_278_308_321_682_708_769_843_864_.csv', '220_263_300_.csv', '835_.csv', '964_.csv', '493_544_.csv', '48_122_132_137_161_172_329_358_408_497_601_755_859_896_940_.csv', '20_47_73_141_229_322_359_420_447_506_577_602_607_699_767_.csv', '563_590_651_663_.csv', '370_.csv', '963_.csv', '620_.csv', '874_.csv', '430_523_550_555_571_.csv', '361_.csv', '382_423_.csv', '106_123_183_207_.csv', '825_.csv', '589_.csv', '526_634_638_658_.csv', '675_723_729_739_761_.csv', '877_890_949_.csv', '585_.csv', '81_109_144_177_260_303_343_352_393_457_.csv', '384_.csv', '331_351_367_437_.csv', '396_.csv', '12_689_721_850_.csv', '182_.csv', '800_861_957_.csv', '202_283_495_.csv', '486_.csv', '536_567_576_691_715_837_.csv', '11_77_97_203_226_237_274_.csv', '591_.csv', '19_94_.csv', '88_127_149_184_327_338_400_474_494_604_667_966_.csv', '641_661_676_684_700_720_.csv', '736_791_886_.csv', '727_753_774_785_831_.csv']\n",
      "Number of valid abnormal files: 55\n",
      "Valid normal files: ['758_.csv', '347_.csv', '143_.csv', '569_.csv', '480_.csv', '5_.csv', '292_.csv', '277_.csv', '16_.csv', '958_.csv', '21_31_42_134_.csv', '14_40_64_.csv', '214_.csv', '336_404_428_434_492_.csv', '452_.csv', '619_.csv', '632_793_888_.csv', '18_.csv', '464_477_562_570_622_625_.csv', '34_.csv', '644_.csv', '1_.csv', '4_.csv', '78_.csv', '592_626_648_.csv', '174_235_.csv', '193_.csv', '114_157_223_238_.csv', '51_.csv', '99_.csv', '955_.csv', '636_694_717_764_.csv', '751_.csv', '826_881_902_929_.csv', '687_.csv', '92_116_121_.csv', '227_.csv', '933_.csv', '135_228_.csv', '397_.csv', '255_.csv', '63_.csv', '200_.csv', '275_.csv', '713_.csv', '43_79_112_120_148_.csv', '511_.csv', '3_.csv', '442_453_.csv', '66_.csv', '789_.csv', '213_.csv', '355_.csv', '335_360_403_637_650_707_.csv', '15_52_.csv', '860_.csv', '938_.csv', '194_317_.csv', '85_.csv', '693_.csv', '609_.csv', '967_.csv', '617_624_.csv', '939_944_.csv', '968_.csv', '8_.csv', '482_498_527_547_578_599_.csv', '6_.csv', '798_817_.csv', '856_.csv', '96_.csv', '852_872_907_917_.csv', '551_574_.csv', '82_.csv', '554_.csv', '324_.csv', '431_.csv', '965_.csv', '390_414_.csv', '108_.csv', '9_76_111_314_432_471_733_752_805_842_857_887_.csv', '209_.csv', '61_.csv', '17_29_90_.csv', '131_.csv', '10_.csv', '87_145_156_171_189_198_.csv', '46_.csv', '748_.csv', '2_.csv', '32_.csv', '368_.csv', '265_309_.csv', '49_.csv', '766_.csv', '660_747_.csv', '605_.csv', '444_502_531_.csv', '895_.csv', '947_.csv', '970_.csv', '378_.csv', '732_812_915_.csv', '342_.csv']\n",
      "Number of valid normal files: 104\n",
      "Processing abnormal files...\n",
      "Processing normal files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: divide by zero encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:715: RuntimeWarning: invalid value encountered in multiply\n",
      "  y_new = slope*(x_new - x_lo)[:, None] + y_lo\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: divide by zero encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:715: RuntimeWarning: invalid value encountered in multiply\n",
      "  y_new = slope*(x_new - x_lo)[:, None] + y_lo\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: divide by zero encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:715: RuntimeWarning: invalid value encountered in multiply\n",
      "  y_new = slope*(x_new - x_lo)[:, None] + y_lo\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: divide by zero encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:715: RuntimeWarning: invalid value encountered in multiply\n",
      "  y_new = slope*(x_new - x_lo)[:, None] + y_lo\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: divide by zero encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:715: RuntimeWarning: invalid value encountered in multiply\n",
      "  y_new = slope*(x_new - x_lo)[:, None] + y_lo\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: divide by zero encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:715: RuntimeWarning: invalid value encountered in multiply\n",
      "  y_new = slope*(x_new - x_lo)[:, None] + y_lo\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:712: RuntimeWarning: divide by zero encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "c:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:715: RuntimeWarning: invalid value encountered in multiply\n",
      "  y_new = slope*(x_new - x_lo)[:, None] + y_lo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 130 trajectories from folder 10.\n",
      "Computing distance matrix...\n",
      "Distance percentiles: [nan nan nan nan]\n",
      "Using eps=nan and min_samples=6\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'eps' parameter of DBSCAN must be a float in the range (0.0, inf). Got np.float64(nan) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 355\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# Apply DBSCAN with precomputed distance matrix\u001b[39;00m\n\u001b[0;32m    350\u001b[0m dbscan \u001b[38;5;241m=\u001b[39m DBSCAN(\n\u001b[0;32m    351\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps_value, \n\u001b[0;32m    352\u001b[0m     min_samples\u001b[38;5;241m=\u001b[39mmin_samples,\n\u001b[0;32m    353\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    354\u001b[0m )\n\u001b[1;32m--> 355\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43mdbscan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# Count clusters\u001b[39;00m\n\u001b[0;32m    358\u001b[0m unique_clusters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(clusters)\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\sklearn\\cluster\\_dbscan.py:474\u001b[0m, in \u001b[0;36mDBSCAN.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute clusters from a data or distance matrix and predict labels.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m        Cluster labels. Noisy samples are given the label -1.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\sklearn\\base.py:1466\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1462\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1463\u001b[0m )\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1466\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\sklearn\\base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\envs\\test\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'eps' parameter of DBSCAN must be a float in the range (0.0, inf). Got np.float64(nan) instead."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.preprocessing import TimeSeriesResampler\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "###############################################################################\n",
    "# Parameters & Paths\n",
    "###############################################################################\n",
    "sample = \"10\"  # Process folder \"10\"\n",
    "folder_path = sample            # Folder containing CSV files for sample 10\n",
    "abnormal_txt_file = f\"AbnormalTracks_{sample}.txt\"  # Text file with abnormal track IDs\n",
    "image_path = f\"sample{sample}.jpg\"  # Background image for visualization (optional)\n",
    "\n",
    "# Create output directory if needed\n",
    "output_dir = \"trajectory_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 1. Read Abnormal Track IDs and Determine Valid Files\n",
    "###############################################################################\n",
    "with open(abnormal_txt_file, \"r\") as file:\n",
    "    abnormal_lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "    abnormal_files_names = [f.split(\".\")[0] for f in abnormal_lines]\n",
    "\n",
    "# List all CSV files available in the folder.\n",
    "all_csv_files = set(os.listdir(folder_path))\n",
    "print(len(all_csv_files), \"CSV files found in folder:\", folder_path)\n",
    "\n",
    "# For each abnormal track ID, perform a prefix match to find a valid CSV file.\n",
    "valid_abnormal_files = []\n",
    "for name in abnormal_files_names:\n",
    "    for csv_file_name in all_csv_files.copy():\n",
    "        if csv_file_name.endswith(\".csv\") and name in csv_file_name:\n",
    "            valid_abnormal_files.append(csv_file_name)\n",
    "            all_csv_files.remove(csv_file_name)  # Remove matched file so remaining are normal\n",
    "            break  # Use first match found\n",
    "\n",
    "# Normal files are those CSVs that are not marked as abnormal.\n",
    "valid_normal_files = [f for f in all_csv_files if f.endswith('.csv') and f not in valid_abnormal_files]\n",
    "\n",
    "print(\"Valid abnormal files:\", valid_abnormal_files)\n",
    "print(\"Number of valid abnormal files:\", len(valid_abnormal_files))\n",
    "print(\"Valid normal files:\", valid_normal_files)\n",
    "print(\"Number of valid normal files:\", len(valid_normal_files))\n",
    "\n",
    "###############################################################################\n",
    "# 2. Define Functions to Extract Trajectories & Preprocess\n",
    "###############################################################################\n",
    "def extract_trajectory(csv_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file (with columns: frameNo, left, top, w, h) and returns the\n",
    "    trajectory as an array of [frameNo, center_x, center_y] values.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, usecols=[\"frameNo\", \"left\", \"top\", \"w\", \"h\"])\n",
    "    df[\"center_x\"] = df[\"left\"] + df[\"w\"] / 2\n",
    "    df[\"center_y\"] = df[\"top\"] + df[\"h\"] / 2\n",
    "    return df[[\"frameNo\", \"center_x\", \"center_y\"]].values\n",
    "\n",
    "def normalize_trajectory(traj):\n",
    "    \"\"\"\n",
    "    Normalize a trajectory by scaling and centering.\n",
    "    Instead of just using the first point, we use the entire trajectory's\n",
    "    bounding box to normalize, preserving the overall shape.\n",
    "    \"\"\"\n",
    "    # Extract just the spatial coordinates\n",
    "    spatial = traj[:, 1:3].copy()\n",
    "    \n",
    "    # Find the bounding box\n",
    "    min_coords = np.min(spatial, axis=0)\n",
    "    max_coords = np.max(spatial, axis=0)\n",
    "    \n",
    "    # Center the trajectory to the middle of its bounding box\n",
    "    center = (min_coords + max_coords) / 2\n",
    "    centered = spatial - center\n",
    "    \n",
    "    # Scale to normalize\n",
    "    scale = np.max(np.abs(centered)) if np.max(np.abs(centered)) > 0 else 1\n",
    "    normalized = centered / scale\n",
    "    \n",
    "    # Return with original frameNo\n",
    "    result = np.column_stack((traj[:, 0], normalized))\n",
    "    return result\n",
    "\n",
    "def resample_trajectory_uniform(traj, n_points=50):\n",
    "    \"\"\"\n",
    "    Resamples the trajectory to a fixed number of points, ensuring even sampling\n",
    "    across the entire path length, not just based on time/frame numbers.\n",
    "    This prevents bias towards the beginning or end of trajectories.\n",
    "    \"\"\"\n",
    "    # First normalize the trajectory\n",
    "    traj_norm = normalize_trajectory(traj)\n",
    "    \n",
    "    # Extract components\n",
    "    frames = traj_norm[:, 0]\n",
    "    x_coords = traj_norm[:, 1]\n",
    "    y_coords = traj_norm[:, 2]\n",
    "    \n",
    "    # Calculate cumulative path length (spatial only)\n",
    "    points = np.column_stack((x_coords, y_coords))\n",
    "    diffs = np.diff(points, axis=0)\n",
    "    segment_lengths = np.sqrt(np.sum(diffs**2, axis=1))\n",
    "    cum_length = np.insert(np.cumsum(segment_lengths), 0, 0)\n",
    "    \n",
    "    if cum_length[-1] == 0:  # Handle stationary trajectories\n",
    "        return np.tile(traj_norm[0], (n_points, 1))\n",
    "    \n",
    "    # Create parameter t based on path length (not frame numbers)\n",
    "    t_orig = cum_length / cum_length[-1]\n",
    "    t_new = np.linspace(0, 1, n_points)\n",
    "    \n",
    "    # Interpolate each coordinate based on path length\n",
    "    if len(t_orig) > 1:  # Only interpolate if we have multiple points\n",
    "        x_interp = interp1d(t_orig, x_coords, kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "        y_interp = interp1d(t_orig, y_coords, kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "        frame_interp = interp1d(t_orig, frames, kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "        \n",
    "        # Create resampled trajectory\n",
    "        new_x = x_interp(t_new)\n",
    "        new_y = y_interp(t_new)\n",
    "        new_frames = frame_interp(t_new)\n",
    "        \n",
    "        return np.column_stack((new_frames, new_x, new_y))\n",
    "    else:\n",
    "        # If only one point, return it repeated\n",
    "        return np.tile(traj_norm[0], (n_points, 1))\n",
    "\n",
    "###############################################################################\n",
    "# 3. Calculate trajectory features to augment the analysis\n",
    "###############################################################################\n",
    "def compute_trajectory_features(traj):\n",
    "    \"\"\"\n",
    "    Compute additional features from a trajectory to help identify abnormal patterns.\n",
    "    Features include:\n",
    "    - Total path length\n",
    "    - Average speed\n",
    "    - Maximum speed\n",
    "    - Straightness (ratio of displacement to path length)\n",
    "    - Number of direction changes beyond a threshold\n",
    "    - Acceleration statistics\n",
    "    \"\"\"\n",
    "    # Extract spatial coordinates\n",
    "    spatial = traj[:, 1:3]\n",
    "    frames = traj[:, 0]\n",
    "    \n",
    "    # Skip if too short\n",
    "    if len(spatial) < 3:\n",
    "        return [0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    # Calculate displacements between consecutive points\n",
    "    displacements = np.diff(spatial, axis=0)\n",
    "    \n",
    "    # Path length (sum of segment lengths)\n",
    "    segment_lengths = np.sqrt(np.sum(displacements**2, axis=1))\n",
    "    total_path_length = np.sum(segment_lengths)\n",
    "    \n",
    "    # Overall displacement (straight-line distance from start to end)\n",
    "    overall_displacement = np.linalg.norm(spatial[-1] - spatial[0])\n",
    "    \n",
    "    # Straightness ratio (1.0 means perfectly straight line)\n",
    "    straightness = overall_displacement / total_path_length if total_path_length > 0 else 0\n",
    "    \n",
    "    # Time intervals between frames\n",
    "    time_intervals = np.diff(frames)\n",
    "    \n",
    "    # Speed calculation (units per frame)\n",
    "    speeds = segment_lengths / time_intervals if len(time_intervals) > 0 else np.zeros_like(segment_lengths)\n",
    "    avg_speed = np.mean(speeds) if len(speeds) > 0 else 0\n",
    "    max_speed = np.max(speeds) if len(speeds) > 0 else 0\n",
    "    \n",
    "    # Direction changes\n",
    "    if len(displacements) >= 2:\n",
    "        # Normalize displacement vectors\n",
    "        displacement_norms = np.sqrt(np.sum(displacements**2, axis=1))\n",
    "        unit_displacements = np.divide(displacements, displacement_norms[:, np.newaxis], \n",
    "                                      where=displacement_norms[:, np.newaxis]>0)\n",
    "        \n",
    "        # Calculate dot products between consecutive displacement vectors\n",
    "        dot_products = np.sum(unit_displacements[:-1] * unit_displacements[1:], axis=1)\n",
    "        \n",
    "        # Clamp to [-1, 1] to avoid numerical issues\n",
    "        dot_products = np.clip(dot_products, -1, 1)\n",
    "        \n",
    "        # Convert to angles in degrees\n",
    "        angles = np.arccos(dot_products) * 180 / np.pi\n",
    "        \n",
    "        # Count significant changes in direction (> 30 degrees)\n",
    "        direction_changes = np.sum(angles > 30)\n",
    "    else:\n",
    "        direction_changes = 0\n",
    "    \n",
    "    # Acceleration calculation (change in speed)\n",
    "    acceleration = np.diff(speeds) if len(speeds) > 1 else np.array([0])\n",
    "    max_acceleration = np.max(np.abs(acceleration)) if len(acceleration) > 0 else 0\n",
    "    avg_acceleration = np.mean(np.abs(acceleration)) if len(acceleration) > 0 else 0\n",
    "    \n",
    "    return [\n",
    "        total_path_length,\n",
    "        avg_speed,\n",
    "        max_speed,\n",
    "        straightness,\n",
    "        direction_changes,\n",
    "        max_acceleration,\n",
    "        avg_acceleration\n",
    "    ]\n",
    "\n",
    "###############################################################################\n",
    "# 4. Load Trajectories & Prepare Data\n",
    "###############################################################################\n",
    "raw_trajectories = []  # Store original trajectories\n",
    "processed_trajectories = []  # Store processed trajectories for clustering\n",
    "trajectory_features = []  # Store additional features\n",
    "labels = []        # 1 for abnormal, 0 for normal (based on file designation)\n",
    "file_names = []    # Store file names for reference\n",
    "\n",
    "# Process abnormal CSV files\n",
    "print(\"Processing abnormal files...\")\n",
    "for file_name in valid_abnormal_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    traj = extract_trajectory(file_path)\n",
    "    if len(traj) > 5:  # Ensure trajectory has enough points\n",
    "        raw_trajectories.append(traj)\n",
    "        resampled = resample_trajectory_uniform(traj, n_points=50)\n",
    "        processed_trajectories.append(resampled)\n",
    "        trajectory_features.append(compute_trajectory_features(traj))\n",
    "        labels.append(1)  # Abnormal\n",
    "        file_names.append(file_name)\n",
    "\n",
    "# Process normal CSV files\n",
    "print(\"Processing normal files...\")\n",
    "for file_name in valid_normal_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    traj = extract_trajectory(file_path)\n",
    "    if len(traj) > 5:  # Ensure trajectory has enough points\n",
    "        raw_trajectories.append(traj)\n",
    "        resampled = resample_trajectory_uniform(traj, n_points=50)\n",
    "        processed_trajectories.append(resampled)\n",
    "        trajectory_features.append(compute_trajectory_features(traj))\n",
    "        labels.append(0)  # Normal\n",
    "        file_names.append(file_name)\n",
    "\n",
    "print(f\"Loaded {len(processed_trajectories)} trajectories from folder {folder_path}.\")\n",
    "\n",
    "# Convert trajectory features to array for analysis\n",
    "trajectory_features = np.array(trajectory_features)\n",
    "feature_names = [\n",
    "    'path_length', 'avg_speed', 'max_speed', 'straightness', \n",
    "    'direction_changes', 'max_acceleration', 'avg_acceleration'\n",
    "]\n",
    "\n",
    "# Visualize feature distributions by label\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, feature in enumerate(feature_names):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    for label, color, name in zip([0, 1], ['blue', 'red'], ['Normal', 'Abnormal']):\n",
    "        idx = np.where(np.array(labels) == label)[0]\n",
    "        if len(idx) > 0:\n",
    "            plt.hist(trajectory_features[idx, i], alpha=0.5, color=color, label=name, bins=10)\n",
    "    plt.xlabel(feature)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, f\"feature_distributions_{sample}.png\"))\n",
    "plt.close()\n",
    "\n",
    "###############################################################################\n",
    "# 5. Compute DTW-based distance matrix for trajectories\n",
    "###############################################################################\n",
    "def compute_dtw_matrix(trajectories, feature_weights=None):\n",
    "    \"\"\"\n",
    "    Compute a distance matrix for trajectories using Dynamic Time Warping (DTW).\n",
    "    Only compares the spatial coordinates (x, y) and ignores the time component.\n",
    "    Optionally incorporates additional feature distances.\n",
    "    \"\"\"\n",
    "    n = len(trajectories)\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    \n",
    "    # Normalize features if provided\n",
    "    if feature_weights is not None:\n",
    "        # Standardize features\n",
    "        feature_means = np.mean(trajectory_features, axis=0)\n",
    "        feature_stds = np.std(trajectory_features, axis=0)\n",
    "        feature_stds[feature_stds == 0] = 1  # Avoid division by zero\n",
    "        norm_features = (trajectory_features - feature_means) / feature_stds\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            # Extract just the spatial dimensions for DTW\n",
    "            seq1 = trajectories[i][:, 1:3]  # x, y coordinates\n",
    "            seq2 = trajectories[j][:, 1:3]  # x, y coordinates\n",
    "            \n",
    "            # Compute DTW distance\n",
    "            dtw_distance = dtw(seq1, seq2)\n",
    "            \n",
    "            # Optionally add weighted feature distance\n",
    "            distance = dtw_distance\n",
    "            if feature_weights is not None:\n",
    "                # Compute Euclidean distance between normalized features\n",
    "                feature_dist = np.linalg.norm(\n",
    "                    (norm_features[i] - norm_features[j]) * feature_weights\n",
    "                )\n",
    "                # Combine DTW and feature distances\n",
    "                distance = dtw_distance + feature_dist\n",
    "            \n",
    "            # Fill the symmetric matrix\n",
    "            dist_matrix[i, j] = distance\n",
    "            dist_matrix[j, i] = distance\n",
    "    \n",
    "    return dist_matrix\n",
    "\n",
    "# You can experiment with different feature weights to emphasize certain aspects\n",
    "# Weights are in the same order as feature_names list\n",
    "feature_weights = np.array([0.2, 0.3, 0.3, 0.5, 0.4, 0.3, 0.3])\n",
    "\n",
    "# Compute distance matrix with DTW + feature weights\n",
    "print(\"Computing distance matrix...\")\n",
    "dist_matrix = compute_dtw_matrix(processed_trajectories, feature_weights=feature_weights)\n",
    "\n",
    "# Visualize the distance matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(dist_matrix, cmap='viridis')\n",
    "plt.colorbar(label='Distance')\n",
    "plt.title('Distance Matrix between Trajectories')\n",
    "plt.savefig(os.path.join(output_dir, f\"distance_matrix_{sample}.png\"))\n",
    "plt.close()\n",
    "\n",
    "###############################################################################\n",
    "# 6. Apply DBSCAN for Clustering using the precomputed distance matrix\n",
    "###############################################################################\n",
    "# The eps value needs to be tuned based on the distribution of distances\n",
    "# Start with a percentile of the distances to get a better estimate\n",
    "distances_flattened = dist_matrix[np.triu_indices(len(dist_matrix), k=1)]\n",
    "eps_percentiles = np.percentile(distances_flattened, [10, 25, 50, 75])\n",
    "print(f\"Distance percentiles: {eps_percentiles}\")\n",
    "\n",
    "# Select eps as the 25th percentile (can be adjusted based on data)\n",
    "eps_value = eps_percentiles[1]  # 25th percentile\n",
    "min_samples = max(3, int(len(processed_trajectories) * 0.05))  # At least 5% of total trajectories\n",
    "\n",
    "print(f\"Using eps={eps_value:.2f} and min_samples={min_samples}\")\n",
    "\n",
    "# Apply DBSCAN with precomputed distance matrix\n",
    "dbscan = DBSCAN(\n",
    "    eps=eps_value, \n",
    "    min_samples=min_samples,\n",
    "    metric='precomputed'\n",
    ")\n",
    "clusters = dbscan.fit_predict(dist_matrix)\n",
    "\n",
    "# Count clusters\n",
    "unique_clusters = np.unique(clusters)\n",
    "print(f\"Found {len(unique_clusters)} clusters (including noise):\")\n",
    "for c in unique_clusters:\n",
    "    count = np.sum(clusters == c)\n",
    "    print(f\"  Cluster {c}: {count} trajectories\")\n",
    "\n",
    "# Predicted labels: trajectories labeled as noise (cluster == -1) are abnormal\n",
    "predicted_labels = [1 if cl == -1 else 0 for cl in clusters]\n",
    "\n",
    "###############################################################################\n",
    "# 7. Evaluate clustering results\n",
    "###############################################################################\n",
    "# Compare to ground truth labels\n",
    "accuracy = np.mean(np.array(predicted_labels) == np.array(labels))\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(labels, predicted_labels)\n",
    "cr = classification_report(labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Classification Report:\")\n",
    "print(cr)\n",
    "\n",
    "# Analyze misclassifications in detail\n",
    "misclassified_indices = np.where(np.array(predicted_labels) != np.array(labels))[0]\n",
    "if len(misclassified_indices) > 0:\n",
    "    print(f\"Found {len(misclassified_indices)} misclassified trajectories:\")\n",
    "    for idx in misclassified_indices:\n",
    "        print(f\"  File: {file_names[idx]}, True: {labels[idx]}, Predicted: {predicted_labels[idx]}\")\n",
    "        \n",
    "        # Analyze features of misclassified trajectories\n",
    "        features = trajectory_features[idx]\n",
    "        print(f\"  Features: {dict(zip(feature_names, features))}\")\n",
    "\n",
    "# Create a DataFrame with results\n",
    "results_df = pd.DataFrame({\n",
    "    'filename': file_names,\n",
    "    'true_label': labels,\n",
    "    'cluster': clusters,\n",
    "    'predicted_label': predicted_labels\n",
    "})\n",
    "\n",
    "# Add features to the results DataFrame\n",
    "for i, name in enumerate(feature_names):\n",
    "    results_df[name] = trajectory_features[:, i]\n",
    "\n",
    "results_df.to_csv(os.path.join(output_dir, f\"clustering_results_{sample}.csv\"), index=False)\n",
    "\n",
    "###############################################################################\n",
    "# 8. Visualize clusters\n",
    "###############################################################################\n",
    "# Color map for clusters (excluding noise)\n",
    "cmap = plt.cm.get_cmap('tab10', max(2, len(unique_clusters)-1))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# First plot the clusters (excluding noise points)\n",
    "for i, traj in enumerate(processed_trajectories):\n",
    "    if clusters[i] == -1:\n",
    "        continue  # Skip noise points for now\n",
    "    \n",
    "    color = cmap(clusters[i] % 10)  # Cycle through 10 colors\n",
    "    plt.plot(traj[:, 1], traj[:, 2], \n",
    "             color=color, alpha=0.5, \n",
    "             label=f\"Cluster {clusters[i]}\" if clusters[i] not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "# Then plot the noise points on top\n",
    "for i, traj in enumerate(processed_trajectories):\n",
    "    if clusters[i] != -1:\n",
    "        continue\n",
    "    \n",
    "    plt.plot(traj[:, 1], traj[:, 2], \n",
    "             'r--', alpha=0.7, linewidth=1,\n",
    "             label=\"Anomaly\" if \"Anomaly\" not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "plt.title(f\"Trajectory Clustering (Sample {sample})\")\n",
    "plt.xlabel(\"Normalized X\")\n",
    "plt.ylabel(\"Normalized Y\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, f\"cluster_visualization_{sample}.png\"))\n",
    "plt.close()\n",
    "\n",
    "###############################################################################\n",
    "# 9. Visualize trajectories on original image (if available)\n",
    "###############################################################################\n",
    "def visualize_trajectories_on_image(image_path, trajectories, clusters, labels, output_path):\n",
    "    \"\"\"\n",
    "    Visualize multiple trajectories on the background image, colored by cluster.\n",
    "    Mark misclassifications with special symbols.\n",
    "    \"\"\"\n",
    "    # Try to read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Image {image_path} not found. Creating a blank canvas.\")\n",
    "        # Create a blank canvas\n",
    "        max_x = max(np.max(traj[:, 1]) for traj in raw_trajectories)\n",
    "        max_y = max(np.max(traj[:, 2]) for traj in raw_trajectories)\n",
    "        image = np.ones((int(max_y) + 100, int(max_x) + 100, 3), dtype=np.uint8) * 255\n",
    "    else:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Color map for clusters\n",
    "    cmap = plt.cm.get_cmap('tab10', max(2, len(np.unique(clusters))-1))\n",
    "    \n",
    "    # Convert clusters to predicted labels\n",
    "    predicted = np.array([1 if c == -1 else 0 for c in clusters])\n",
    "    \n",
    "    # Plot each trajectory\n",
    "    for i, traj in enumerate(trajectories):\n",
    "        # Get points and convert to integer coordinates\n",
    "        points = np.array([(int(p[1]), int(p[2])) for p in traj])\n",
    "        \n",
    "        # Skip if no points\n",
    "        if len(points) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Determine color based on cluster\n",
    "        if clusters[i] == -1:\n",
    "            color = (255, 0, 0)  # Red for anomalies\n",
    "            thickness = 2\n",
    "        else:\n",
    "            # Convert colormap value to BGR for OpenCV\n",
    "            rgb = cmap(clusters[i] % 10)[:3]\n",
    "            color = tuple(int(c * 255) for c in rgb)\n",
    "            thickness = 1\n",
    "            \n",
    "        # Draw the trajectory line\n",
    "        for j in range(len(points) - 1):\n",
    "            cv2.line(image, points[j], points[j+1], color, thickness)\n",
    "        \n",
    "        # Mark start points\n",
    "        cv2.circle(image, points[0], 5, color, -1)\n",
    "        \n",
    "        # Mark end points and misclassifications\n",
    "        if predicted[i] != labels[i]:\n",
    "            # Mark misclassifications with an X\n",
    "            end_pt = points[-1]\n",
    "            size = 8\n",
    "            cv2.line(image, (end_pt[0]-size, end_pt[1]-size), \n",
    "                     (end_pt[0]+size, end_pt[1]+size), (0, 255, 255), 2)\n",
    "            cv2.line(image, (end_pt[0]-size, end_pt[1]+size), \n",
    "                     (end_pt[0]+size, end_pt[1]-size), (0, 255, 255), 2)\n",
    "        else:\n",
    "            # Normal endpoint marker\n",
    "            cv2.circle(image, points[-1], 5, (0, 0, 0), -1)\n",
    "    \n",
    "    # Add a legend\n",
    "    legend_y = 30\n",
    "    cv2.putText(image, \"Anomaly\", (20, legend_y), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.7, (255, 0, 0), 2)\n",
    "    legend_y += 30\n",
    "    cv2.putText(image, \"Misclassified\", (20, legend_y), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.7, (0, 255, 255), 2)\n",
    "    \n",
    "    for c in np.unique(clusters):\n",
    "        if c == -1:\n",
    "            continue\n",
    "        legend_y += 30\n",
    "        rgb = cmap(c % 10)[:3]\n",
    "        color = tuple(int(c * 255) for c in rgb)\n",
    "        cv2.putText(image, f\"Cluster {c}\", (20, legend_y), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.7, color, 2)\n",
    "    \n",
    "    # Save the result\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Visualize trajectories on image if available\n",
    "try:\n",
    "    output_path = os.path.join(output_dir, f\"trajectory_visualization_{sample}.png\")\n",
    "    visualize_trajectories_on_image(image_path, raw_trajectories, clusters, labels, output_path)\n",
    "    print(f\"Visualization saved to {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create visualization on image: {str(e)}\")\n",
    "\n",
    "###############################################################################\n",
    "# 10. Parameter tuning helper function \n",
    "###############################################################################\n",
    "def tune_dbscan_parameters(dist_matrix, labels, eps_range=None, min_samples_range=None):\n",
    "    \"\"\"\n",
    "    Tune DBSCAN parameters by trying different combinations of eps and min_samples.\n",
    "    Returns a DataFrame with results for each parameter combination.\n",
    "    \"\"\"\n",
    "    if eps_range is None:\n",
    "        # Use percentiles of the distance distribution\n",
    "        distances = dist_matrix[np.triu_indices(len(dist_matrix), k=1)]\n",
    "        eps_range = np.percentile(distances, [10, 15, 20, 25, 30, 35, 40, 50])\n",
    "    \n",
    "    if min_samples_range is None:\n",
    "        # Try different percentages of the total number of samples\n",
    "        n_samples = len(labels)\n",
    "        min_samples_range = [max(2, int(n_samples * pct)) for pct in [0.01, 0.02, 0.05, 0.1, 0.15]]\n",
    "    \n",
    "    results = []\n",
    "    for eps in eps_range:\n",
    "        for min_samples in min_samples_range:\n",
    "            # Apply DBSCAN\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "            clusters = dbscan.fit_predict(dist_matrix)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            predicted = [1 if c == -1 else 0 for c in clusters]\n",
    "            accuracy = np.mean(np.array(predicted) == np.array(labels))\n",
    "            n_clusters = len(np.unique(clusters)) - (1 if -1 in clusters else 0)\n",
    "            noise_ratio = np.sum(clusters == -1) / len(clusters)\n",
    "            \n",
    "            # Calculate precision, recall, and F1 score for abnormal class\n",
    "            true_positives = np.sum((np.array(labels) == 1) & (np.array(predicted) == 1))\n",
    "            false_positives = np.sum((np.array(labels) == 0) & (np.array(predicted) == 1))\n",
    "            false_negatives = np.sum((np.array(labels) == 1) & (np.array(predicted) == 0))\n",
    "            \n",
    "            precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            # Append results\n",
    "            results.append({\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'n_clusters': n_clusters,\n",
    "                'noise_ratio': noise_ratio,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run parameter tuning to find the best parameters\n",
    "print(\"Running parameter tuning...\")\n",
    "tuning_results = tune_dbscan_parameters(dist_matrix, labels)\n",
    "tuning_results.to_csv(os.path.join(output_dir, f\"parameter_tuning_{sample}.csv\"), index=False)\n",
    "\n",
    "# Find best parameters by F1 score (better for imbalanced classes)\n",
    "best_params = tuning_results.loc[tuning_results['f1_score'].idxmax()]\n",
    "print(\"Best parameters by F1 score:\")\n",
    "print(best_params)\n",
    "\n",
    "# Re-run with best parameters\n",
    "best_eps = best_params['eps']\n",
    "best_min_samples = int(best_params['min_samples'])\n",
    "print(f\"Re-running with best parameters: eps={best_eps:.4f}, min_samples={best_min_samples}\")\n",
    "\n",
    "# Apply DBSCAN with best parameters\n",
    "best_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples, metric='precomputed')\n",
    "best_clusters = best_dbscan.fit_predict(dist_matrix)\n",
    "best_predicted = [1 if c == -1 else 0 for c in best_clusters]\n",
    "\n",
    "# Evaluate final results\n",
    "final_accuracy = np.mean(np.array(best_predicted) == np.array(labels))\n",
    "final_cm = confusion_matrix(labels, best_predicted)\n",
    "final_cr = classification_report(labels, best_predicted)\n",
    "\n",
    "print(f\"Final accuracy: {final_accuracy:.4f}\")\n",
    "print(\"Final confusion matrix:\")\n",
    "print(final_cm)\n",
    "print(\"Final classification report:\")\n",
    "print(final_cr)\n",
    "\n",
    "# Save final results\n",
    "final_results = pd.DataFrame({\n",
    "    'filename': file_names,\n",
    "    'true_label': labels,\n",
    "    'cluster': best_clusters,\n",
    "    'predicted_label': best_predicted\n",
    "})\n",
    "\n",
    "# Add features to the results\n",
    "for i, name in enumerate(feature_names):\n",
    "    final_results[name] = trajectory_features[:, i]\n",
    "\n",
    "final_results.to_csv(os.path.join(output_dir, f\"final_results_{sample}.csv\"), index=False)\n",
    "\n",
    "# Final visualization with best parameters\n",
    "try:\n",
    "    output_path = os.path.join(output_dir, f\"final_visualization_{sample}.png\")\n",
    "    visualize_trajectories_on_image(image_path, raw_trajectories, best_clusters, labels, output_path)\n",
    "    print(f\"Final visualization saved to {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create final visualization: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
